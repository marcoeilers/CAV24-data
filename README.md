This repository contains all relevant data for the evaluation of the CAV 2024 paper "Verification Algorithms for Automated Separation Logic Verifiers".

## Structure

The data in this repository is structured as follows:

- Directory ``benchmarks`` contains the examples (i.e., all Viper files) that were benchmarked.
  - Subdirectory ``examples_all`` contains all benchmarks, ordered by the group they belong to.
    Note that not all files were included in the evaluation as separate examples;
    some files only contain definitions and are used by other files, and some files were manually excluded because they did not contain any meaninful verification task by themselves or because they were inherently problematic (e.g. because of bad quantifier patterns).
  - Subdirectory ``examples_afvo`` contain a small number of examples for which the SE-based verifiers have to be run with different settings.
    In particular, the used SE engine verifies functions in an order that is incomplete for these files, but has a flag (AlternativeFunctionVerificationOrder) to force a better verification order.
    Since function verification order is independent of SE or VCG and this is an unfortunate implementation detail, we re-benchmark these files with said flag to get meaningful results.
  - Subdirectory ``warmup`` contains files that were verified to warm up the used JVM before benchmarking the actual examples.
- Directory ``results`` contains the raw results of the benchmarking process.
  - Subdirectory ``measured`` contains CSV files that were the direct result of running all five implementations plus an unoptimized version of SE-PS on the examples.
    That means it contains full runs with standard settings, split into two parts for SE-PS-unoptimized because this implementation ran out of memory on an example, forcing us to run the benchmarks in two parts,
    as well as an additional run on the AFVO files with that flag set (see above).
  - Subdirectory ``consolidated`` contains files that consolidate this data into one CSV file for each implementation.
  - Subdirectory ``collected`` contains two files:
    - ``Benchmarks.ods`` is the main file that collects all runtimes for all implementations and is discussed in more detail below.
    - ``Benchmarks.csv`` is a CSV version of the first sheet of ``Benchmarks.ods`` and is used as input for the analysis, see below.
- Directory ``analysis`` contains
  - The Jupyter notebook ``analysis.ipynb`` that performs all analysis
  - ``benchmark_data.csv``, which is an exact copy of ``results/collected/Benchmarks.csv`` and is used as input by the Jupyter notebook.
  - Four directories that are generated by ``analysis.ipynb`` and are also discussed below.

## Benchmarks.ods

``Benchmarks.ods`` is the main file that collects all runtimes for all implementations, and contains the following data:
- One sheet for each implementation contains the benchmark data for all examples for each implementation. This data is copy-pasted exactly from the relevant CSV file in ``results/consolidated``.
- In sheet 1, all data is collected:
  - Column 1 contains the names and paths of all examples that we used in the benchmark (since, as stated above, not all files in ``benchmarks/examples_all`` were included).
  - Column 2 contains the expected verification result for each example.
  - The following columns contain, for each implementation
    - ``Mean``: the mean time for each algorithm across five runs (after eliminating the two most extreme runs, as described in the paper)
    - ``Consistent``: whether the reported result was consistent across all five runs
    - ``Result``: the reported result
  - Additionally, there are ``Notes`` columns for each implementation that log some additional data. In particular, the statements in the paper about some groups having a lot of branching is based on this data and the calculation in the following columns.

## analysis.ipynb

This is the main file that reads the benchmark data in ``benchmark_data.csv`` and computes completeness numbers, portfolios, and generates the box plots shown in the paper.
In particular:
- It outputs some data starting with "Completeness-relevant data" that contains the absolute numbers that form the basis of Table 1 in the paper (but transposed).
  - The first two lines contain all gorups and the number of examples they contain
  - Then, there are two lines for each algorithm. The first represents, for each group, how many examples were incomplete. The second, how many of those incompletenesses were due to timeouts or inconsistent results. Table 1 in the papers shows these numbers converted to percentages of the number of examples per group.
- The section below ``Computing portfolios...`` contains identified portfolio that is complete for all examples, and for all portfolios that are not complete for all examples, the number for which they are incomplete and the names of the incomplete examples.
- The code below that generates the data discussed below.

## Generated analysis data

- Subdirectory ``boxplots``, which contains the performance box plots show in the paper (and others that compare pairs of algorithms and portfolios not shown in the paper).
- Subdirectory ``completeness_excels``, which show a detailed (per-example) comparison of completeness differences for each pair of implementations.
- Subdirectory ``runtime_excels``, which show a detailed, per-example performance comparoson for the relevant examples for each pair of implementations.
- Subdirectory ``logging``, which contains various kinds of intermediate data.
    
