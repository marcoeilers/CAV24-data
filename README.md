This repository contains all relevant data for the evaluation of the CAV 2024 paper "Verification Algorithms for Automated Separation Logic Verifiers".

## Structure

The data in this repository is structured as follows:

- Directory ``benchmarks`` contains the examples (i.e., all Viper files) that were benchmarked.
  - Subdirectory ``examples_all`` contains all benchmarks, ordered by the group they belong to.
    Note that not all files were included in the evaluation as separate examples;
    some files only contain definitions and are used by other files, and some files were manually excluded because they did not contain any meaninful verification task by themselves or because they were inherently problematic (e.g. because of bad quantifier patterns).
  - Subdirectory ``examples_afvo`` contain a small number of examples for which the SE-based verifiers have to be run with different settings.
    In particular, the used SE engine verifies functions in an order that is incomplete for these files, but has a flag (AlternativeFunctionVerificationOrder) to force a better verification order.
    Since function verification order is independent of SE or VCG and this is an unfortunate implementation detail, we re-benchmark these files with said flag to get meaningful results.
  - Subdirectory ``warmup`` contains files that were verified to warm up the used JVM before benchmarking the actual examples.
- Directory ``results`` contains the raw results of the benchmarking process.
  - Subdirectory ``measured`` contains CSV files that were the direct result of running all five implementations plus an unoptimized version of SE-PS on the examples.
    That means it contains full runs with standard settings, split into two parts for SE-PS-unoptimized because this implementation ran out of memory on an example, forcing us to run the benchmarks in two parts,
    as well as an additional run on the AFVO files with that flag set (see above).
  - Subdirectory ``consolidated`` contains files that consolidate this data into one CSV file for each implementation.
  - Subdirectory ``collected`` contains two files:
    - ``Benchmarks.ods`` is the main file that collects all runtimes for all implementations and is discussed in more detail below.
    - ``Benchmarks.csv`` is a CSV version of the first sheet of ``Benchmarks.ods`` and is used as input for the analysis, see below.
- Directory ``analysis`` contains
  - The Jupyter notebook ``analysis.ipynb`` that performs all analysis
  - ``benchmark_data.csv``, which is an exact copy of ``results/collected/Benchmarks.csv`` and is used as input by the Jupyter notebook.
  - Four directories that are generated by ``analysis.ipynb`` and are also discussed below.

## Benchmarks.ods

## analysis.ipynb

## Generated analysis data

- Subdirectory ``boxplots``, which contains the performance box plots show in the paper (and others that compare pairs of algorithms and portfolios not shown in the paper).
- Subdirectory ``logging``, which
- Subdirectory ``runtime_excels``, which
- Subdirectory ``completeness_excels``, which
    
